/*
What is TIME TRAVEL and how it is used in SNOWFLAKE?
-- Snowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. 
-- It serves as a powerful tool for performing the following tasks:
    - Restoring data-related objects (tables, schemas, and databases) that may have been accidentally or intentionally deleted.
    - Duplicating and backing up data from key points in the past.
    - Analyzing data usage/manipulation over specified periods of time.
*/

-- SET DATA_RETENTION_TIME_IN_DAYS PROPERTY FOR TIME TRAVEL

create or replace table employees(employee_id number,
                     salary number,
                     manager_id number)
                   data_retention_time_in_days=90;
                     
SHOW TABLES;
                     
create or replace table employees_test(employee_id number,
                     salary number,
                     manager_id number)
                     data_retention_time_in_days=95;
                     
alter table employees set data_retention_time_in_days=30;

SHOW TABLES;


-- If retention period is changed for account or individual objects then retention period will be changed for all lower level objects as well unless explicitly set

USE SCHEMA EMPLOYEE_PERM;

create or replace table employees_new(employee_id number,
                     salary number,
                     manager_id number)
                   data_retention_time_in_days=90;
                     
insert into employees values(8,40000,4),
                                 (12,50000,9),
                                 (3,30000,5),
                                 (4,10000,5),
                                 (25,35000,9),
                                 (12,50000,9),
                                 (86,90000,4),
                                 (73,20000,1);
                     
SHOW TABLES;




create or replace transient table employees_transient(employee_id number,
                     salary number,
                     manager_id number);
                     
create or replace temporary table employees_temp(employee_id number,
                     salary number,
                     manager_id number);
                     
show tables;


SHOW SCHEMAS;

SELECT * FROM DEMO_DB.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'EMPLOYEE_PERM';

ALTER SCHEMA DEMO_DB.EMPLOYEE_PERM set data_retention_time_in_days=55;

SHOW SCHEMAS;

SELECT * FROM DEMO_DB.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'EMPLOYEE_PERM';


-------====================================================================================================================;


-- QUERY HISTORICAL DATA

-- query selects historical data from a table as of the date and time represented by the specified timestamp:
Delete from employees where employee_id=25; -- After this you won't able to see the data for this perticular id=25;

select current_timestamp();

ALTER SESSION SET TIMEZONE = 'UTC';

select * from employees before(timestamp => '2020-09-10 20:50:28.944 +0000'::timestamp);


-- query selects historical data from a table as of 5 minutes ago:

select * from employees at(offset => -60*7);


-- query selects historical data from a table up to, but not including any changes made by the specified statement:

select * from employees before(statement => '0196d7be-002d-94a5-0000-45750002d276');

select * from employees before(statement => '0196d7b7-0046-2872-0000-45750002b1be');

-----------


-------====================================================================================================================

-- CLONE HISTORICAL OBJECTS

-- create a duplicate of the object at a specified point in the object’s history

select current_timestamp();


-- following CREATE TABLE command creates a clone of a table as of the date and time represented by the specified timestamp:
create table restored_table clone employees
  at(timestamp => '2020-09-10 21:06:16.694 +0000'::timestamp);
  
-- following CREATE SCHEMA command creates a clone of a schema and all its objects as they existed 1 hour before the current time:
create schema restored_schema clone employee_perm at(offset => -600);

-- following CREATE DATABASE command creates a clone of a database and all its objects as they existed prior to the completion of the specified statement:
create database restored_db clone demo_db
  before(statement => '0196d7b8-00d6-37a7-0000-45750002d1ce');
  
  

-------====================================================================================================================  

  /*

-- DROPPING AND RESTORING OBJECTS --

When a table, schema, or database is dropped, it is not immediately overwritten or removed from the system. 
Instead, it is retained for the data retention period for the object, during which time the object can be restored. 
Once dropped objects are moved to Fail-safe, you cannot restore them.

*/

show tables history like 'employees%' in demo_db.public;

show schemas history in demo_db;

show databases history;

drop database development;

drop schema demo_db.employee;

drop table demo_db.employee_perm.employees;

-- The output includes all dropped objects and an additional DROPPED_ON column, which displays the date and time when the object was dropped. 
-- If an object has been dropped more than once, each version of the object is included as a separate row in the output.


/*

-- RESTORING OBJECTS --
A dropped object that has not been purged from the system (i.e. the object is displayed in the SHOW <object_type> HISTORY output) 
can be restored using the following commands:

Calling UNDROP restores the object to its most recent state before the DROP command was issued.

*/

undrop table demo_db.employee_perm.employees;

undrop schema demo_db.employee;

undrop database development;

-- If an object with the same name already exists, UNDROP fails. 
-- You must rename the existing object, which then enables you to restore the previous version of the object.


whats is external volume

=======================================
=======================================
=======================================
=======================================
=======================================

-- Create table to load CSV data
CREATE or replace TABLE HEALTHCARE_CSV(
    AVERAGE_COVERED_CHARGES    NUMBER(38,6)  
   ,AVERAGE_TOTAL_PAYMENTS    NUMBER(38,6)  
   ,TOTAL_DISCHARGES    NUMBER(38,0)  
   ,BACHELORORHIGHER    NUMBER(38,1)  
   ,HSGRADORHIGHER    NUMBER(38,1)  
   ,TOTALPAYMENTS    VARCHAR(128)  
   ,REIMBURSEMENT    VARCHAR(128)  
   ,TOTAL_COVERED_CHARGES    VARCHAR(128) 
   ,REFERRALREGION_PROVIDER_NAME    VARCHAR(256)  
   ,REIMBURSEMENTPERCENTAGE    NUMBER(38,9)  
   ,DRG_DEFINITION    VARCHAR(256)  
   ,REFERRAL_REGION    VARCHAR(26)  
   ,INCOME_PER_CAPITA    NUMBER(38,0)  
   ,MEDIAN_EARNINGSBACHELORS    NUMBER(38,0)  
   ,MEDIAN_EARNINGS_GRADUATE    NUMBER(38,0)  
   ,MEDIAN_EARNINGS_HS_GRAD    NUMBER(38,0)  
   ,MEDIAN_EARNINGSLESS_THAN_HS    NUMBER(38,0)  
   ,MEDIAN_FAMILY_INCOME    NUMBER(38,0)  
   ,NUMBER_OF_RECORDS    NUMBER(38,0)  
   ,POP_25_OVER    NUMBER(38,0)  
   ,PROVIDER_CITY    VARCHAR(128)  
   ,PROVIDER_ID    NUMBER(38,0)  
   ,PROVIDER_NAME    VARCHAR(256)  
   ,PROVIDER_STATE    VARCHAR(128)  
   ,PROVIDER_STREET_ADDRESS    VARCHAR(256)  
   ,PROVIDER_ZIP_CODE    NUMBER(38,0)  
);

--Create integration object for external stage
create or replace storage integration s3_int
  type = external_stage
  storage_provider = s3
  enabled = true
  storage_aws_role_arn = 'arn:aws:iam::435098453023:role/snowflake-role'
  storage_allowed_locations = ('s3://testsnowflake/snowflake/', 's3://testxyzsnowflake/');

--Describe integration object to fetch external_id and to be used in s3
DESC INTEGRATION s3_int;

create or replace file format demo_db.public.csv_format
                    type = csv
                    field_delimiter = '|'
                    skip_header = 1
                    null_if = ('NULL', 'null')
                    empty_field_as_null = true;
                    
create or replace stage demo_db.public.ext_csv_stage
  URL = 's3://testsnowflake/snowflake/csv'
  STORAGE_INTEGRATION = s3_int
  file_format = demo_db.public.csv_format;

-- Use copy command to ingest data from S3
copy into healthcare_csv
from @demo_db.public.ext_csv_stage
on_error = CONTINUE;

select * from healthcare_csv;












======================================

/* 
What is Snowpipe?
-- Snowpipe enables loading data from files as soon as they’re available in a stage. 
-- This means you can load data from files in micro-batches, making it available to users within minutes, 
   rather than manually executing COPY statements on a schedule to load larger batches.
*/

-- Create table to load CSV data
CREATE or replace TABLE HEALTHCARE_CSV(
    AVERAGE_COVERED_CHARGES    NUMBER(38,6)  
   ,AVERAGE_TOTAL_PAYMENTS    NUMBER(38,6)  
   ,TOTAL_DISCHARGES    NUMBER(38,0)  
   ,BACHELORORHIGHER    NUMBER(38,1)  
   ,HSGRADORHIGHER    NUMBER(38,1)  
   ,TOTALPAYMENTS    VARCHAR(128)  
   ,REIMBURSEMENT    VARCHAR(128)  
   ,TOTAL_COVERED_CHARGES    VARCHAR(128) 
   ,REFERRALREGION_PROVIDER_NAME    VARCHAR(256)  
   ,REIMBURSEMENTPERCENTAGE    NUMBER(38,9)  
   ,DRG_DEFINITION    VARCHAR(256)  
   ,REFERRAL_REGION    VARCHAR(26)  
   ,INCOME_PER_CAPITA    NUMBER(38,0)  
   ,MEDIAN_EARNINGSBACHELORS    NUMBER(38,0)  
   ,MEDIAN_EARNINGS_GRADUATE    NUMBER(38,0)  
   ,MEDIAN_EARNINGS_HS_GRAD    NUMBER(38,0)  
   ,MEDIAN_EARNINGSLESS_THAN_HS    NUMBER(38,0)  
   ,MEDIAN_FAMILY_INCOME    NUMBER(38,0)  
   ,NUMBER_OF_RECORDS    NUMBER(38,0)  
   ,POP_25_OVER    NUMBER(38,0)  
   ,PROVIDER_CITY    VARCHAR(128)  
   ,PROVIDER_ID    NUMBER(38,0)  
   ,PROVIDER_NAME    VARCHAR(256)  
   ,PROVIDER_STATE    VARCHAR(128)  
   ,PROVIDER_STREET_ADDRESS    VARCHAR(256)  
   ,PROVIDER_ZIP_CODE    NUMBER(38,0)  
);

--Create integration object for external stage
create or replace storage integration s3_int
  type = external_stage
  storage_provider = s3
  enabled = true
  storage_aws_role_arn = 'arn:aws:iam::435098453023:role/snowflake-role'
  storage_allowed_locations = ('s3://testsnowflake/snowflake/', 's3://testxyzsnowflake/');

--Describe integration object to fetch external_id and to be used in s3
DESC INTEGRATION s3_int;

create or replace file format demo_db.public.csv_format
                    type = csv
                    field_delimiter = '|'
                    skip_header = 1
                    null_if = ('NULL', 'null')
                    empty_field_as_null = true;
                    
create or replace stage demo_db.public.ext_csv_stage
  URL = 's3://testsnowflake/snowflake/csv'
  STORAGE_INTEGRATION = s3_int
  file_format = demo_db.public.csv_format;

--create pipe to automate data ingestion from s3 to snowflake
create or replace pipe demo_db.public.mypipe auto_ingest=true as
copy into healthcare_csv
from @demo_db.public.ext_csv_stage
on_error = CONTINUE;

show pipes;

select * from healthcare_csv;


========
======
====



CREATE TABLE EMPLOYEES(EMPLOYEE_ID INTEGER AUTOINCREMENT START = 1 INCREMENT = 1,
                       EMPLOYEE_NAME VARCHAR DEFAULT 'KASHISH',
                       LOAD_TIME DATE);


CREATE OR REPLACE TASK EMPLOYEES_TASK
  WAREHOUSE = COMPUTE_WH
  SCHEDULE = '1 MINUTE'
AS
 INSERT INTO EMPLOYEES(LOAD_TIME) VALUES(CURRENT_TIMESTAMP);
 

SHOW TASKS;


ALTER TASK EMPLOYEES_TASK RESUME;
ALTER TASK EMPLOYEES_TASK SUSPEND;
================================
===============================
================================



CREATE TABLE EMPLOYEES(EMPLOYEE_ID INTEGER AUTOINCREMENT START = 1 INCREMENT = 1,
                       EMPLOYEE_NAME VARCHAR DEFAULT 'KASHISH',
                       LOAD_TIME DATE);


CREATE OR REPLACE TASK EMPLOYEES_TASK
  WAREHOUSE = COMPUTE_WH
  SCHEDULE = '1 MINUTE'
AS
 INSERT INTO EMPLOYEES(LOAD_TIME) VALUES(CURRENT_TIMESTAMP);
 

SHOW TASKS;


ALTER TASK EMPLOYEES_TASK RESUME;
ALTER TASK EMPLOYEES_TASK SUSPEND;



-- First Copy of Employees table
CREATE TABLE EMPLOYEES_COPY(EMPLOYEE_ID INTEGER,
                       EMPLOYEE_NAME VARCHAR,
                       LOAD_TIME DATE);

CREATE OR REPLACE TASK EMPLOYEES_COPY_TASK
  WAREHOUSE = COMPUTE_WH
  AFTER EMPLOYEES_TASK
AS
INSERT INTO EMPLOYEES_COPY(EMPLOYEE_ID, EMPLOYEE_NAME, LOAD_TIME) SELECT * FROM EMPLOYEES;


-- Second Copy of Employees table
CREATE TABLE EMPLOYEES_COPY2(EMPLOYEE_ID INTEGER,
                       EMPLOYEE_NAME VARCHAR,
                       LOAD_TIME DATE);

CREATE or replace TASK EMPLOYEES_COPY_TASK2
  WAREHOUSE = COMPUTE_WH
  AFTER EMPLOYEES_TASK
AS
INSERT INTO EMPLOYEES_COPY2(EMPLOYEE_ID, EMPLOYEE_NAME, LOAD_TIME) SELECT * FROM EMPLOYEES;

ALTER TASK EMPLOYEES_TASK RESUME;
ALTER TASK EMPLOYEES_COPY_TASK RESUME;
ALTER TASK EMPLOYEES_COPY_TASK2 RESUME;

ALTER TASK EMPLOYEES_TASK SUSPEND;
ALTER TASK EMPLOYEES_COPY_TASK SUSPEND;
ALTER TASK EMPLOYEES_COPY_TASK2 SUSPEND;


truncate table employees;



===============
===============
=========


CREATE OR REPLACE TABLE EMPLOYEES(EMPLOYEE_ID INTEGER AUTOINCREMENT START = 1 INCREMENT = 1,
                       EMPLOYEE_NAME VARCHAR DEFAULT 'KASHISH',
                       LOAD_TIME DATE);


-- Stored procedure that INSERTS data TO a table
-- The INSERT statement in the stored procedure COPIES data to EMPLOYEES table
create or replace procedure load_employees_data(TODAY_DATE varchar)
  returns string not null
  language javascript
  as
  $$
    var sql_command = 'INSERT INTO EMPLOYEES(LOAD_TIME) VALUES(:1);'
    snowflake.execute(
        { 
        sqlText: sql_command, 
        binds: [TODAY_DATE] 
        }
        ); 
  return "SUCCEEDED"; 
  $$;

-- Task that calls the stored procedure every minute
create or replace task employees_load_task
  warehouse = COMPUTE_WH
  schedule = '1 minute'
as
  call load_employees_data(CURRENT_TIMESTAMP);
  

ALTER TASK employees_load_task RESUME;
ALTER TASK employees_load_task SUSPEND;



desc task employees_load_task;



TRUNCATE TABLE EMPLOYEES;

SHOW TASKS;

===========================================
==========================================
===========================================



-- Creating temporary table to show that cloning just works for Permanent & Transient tables and not temporary

create or replace TEMPORARY table EMPLOYEES_TEMP(employee_id number,
                     empl_join_date date,
                     dept varchar(10),
                     salary number,
                     manager_id number);
                                          
insert into EMPLOYEES_TEMP values(8,'2014-10-01','HR',40000,4),
                                 (12,'2014-09-01','Tech',50000,9),
                                 (3,'2018-09-01','Marketing',30000,5),
                                 (4,'2017-09-01','HR',10000,5),
                                 (25,'2019-09-01','HR',35000,9),
                                 (12,'2014-09-01','Tech',50000,9),
                                 (86,'2015-09-01','Tech',90000,4),
                                 (73,'2016-09-01','Marketing',20000,1);


update employees set employee_id = 73 where employee_id = 37;

show stages;
list @EXT_CSV_STAGE;

show pipes;

show streams;
select * from EMPLOYEES_STREAM;

show tasks;


-- Clone a database and all objects within the database at its current state:

create or replace database demo_db_clone clone demo_db;

use demo_db_clone;

show stages;
list @EXT_CSV_STAGE;

show pipes;

show streams;
select * from EMPLOYEES_STREAM;

show tasks;

======================
==========================
====================



----------------- DATA SHARING -----------------
-- Create a share
-- Make sure to use ACCOUNTADMIN role or any other role which has CREATE SHARE privilege
create share employees_share;

------- Access Privileges for Shares ------
-- Grant respective privileges before sharing data with any consumer account
-- Grants should be provided from top to granular level objects i.e. Database -> Schemas -> Tables
grant usage on database demo_db to share employees_share;

grant usage on schema demo_db.public to share employees_share;

grant select on table demo_db.public.new_orders to share employees_share;
grant all privileges on table demo_db.public.new_orders to share employees_share;

--=====----

-- Confirm all the grants
show grants to share employees_share;


-- Once all the privileges are granted to the share, now we are ready to share our data to other ACCOUNTS
alter share employees_share add accounts = ce52002;
// Question -> Why we got error??
alter share employees_share add accounts = ey66391;
-- This can also be done using WEB UI


============================================


------------------- General Limitations for Shared Databases ------------------- 
/*
-- Shared databases have the following limitations for consumers:

- Shared databases are read-only. 
- Users in a consumer account can view/query data, but cannot insert or update data, or create any objects in the database.
(DML operations not allowed)

The following actions are not supported:
- Creating a clone of a shared database or any schemas/tables in the database.
- Time Travel for a shared database or any schemas/tables in the database.
- Editing the comments for a shared database.
- Shared databases and all the objects in the database cannot be forwarded (i.e. re-shared with other accounts).
*/

show shares;

desc share QC91829.EMPLOYEES_SHARE;
-- In the results, NAME is prefixed with <DB>. This indicates a database has not been created yet from the share.

create database TEST_SHARED_DB from share QC91829.EMPLOYEES_SHARE;

show databases like 'TEST_SHARED_DB%';

-- Now the database is created so we will be able to see fully qualified name of the objects
show shares;

-- Managing grants and privileges
grant imported privileges on database TEST_SHARED_DB to role sysadmin;

show grants to role sysadmin;

-- Verify that clones cannot be created using shared database
create database cloned_shared_db clone test_shared_db;
// Error: SQL compilation error: Cannot clone from a database that was imported from a share.



 
 
 
 
 
 shared database can not be cloned
 
 because when we clone something it will take reference from  cloud service layer from metadata stored
 
